{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUM 2023-24 Gradientowe uczenie sieci neuronowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problemy\n",
    "<img style=\"float: right;\" src=\"ml_figures/sgd-convergence-divergence.gif\" width=450>\n",
    "\n",
    "1. sieci głębokie działają wolniej i mają więcej parametrów\n",
    "2. silniejszy model - większy overfitting\n",
    "3. wiele __minimów lokalnych__\n",
    "  * model liniowy z wypukłą funkcją kosztu ma tylko jedno __minimum globalne__\n",
    "    * podążając w kierunku przeciwnym do gradientu w końcu osiągniemy minimum\n",
    "  * sieci neuronowe mają bardzo dużo minimów lokalnych\n",
    "    * optymalizacja gradientowa nie umie wydostać się z minimum lokalnego\n",
    "    * prawdopodobnie nigdy nie osiągniemy minimum globalnego\n",
    "4. problemy z backpropagation w warstwowym modelu\n",
    "  * warstwy uczą się z __różną prędkością__\n",
    "  * np. głębsze warstwy uczą się bardzo powoli, a ostatnie warstwy bardzo szybko próbują rozwiązać problem\n",
    "    * słabe wyniki\n",
    "    * nie korzystamy w ogóle z głębokiej struktury\n",
    "  * może być też zupełnie na odwrót\n",
    "5. __vanishing gradient__\n",
    "  * w algorytmie backpropagation delta musi przejść od najwyższej do najniższej warstwy\n",
    "  * w każdej warstwie jest szansa, że kawałek gradientu zostanie (prawie) wyzerowany\n",
    "    * np. jeśli funkcja aktywacji saturuje\n",
    "    * albo dla ujemnych wartości w funkcji ReLU\n",
    "  * im głębiej, tym bardziej efekty te się nawarstwiają\n",
    "  * w najgłębszej warstwie może okazać się, że gradient jest bardzo mały\n",
    "  * mały gradient oznacza, że warstwa ma mały wpływ na output sieci\n",
    "    * dopiero wyższe warstwy mają istotny wpływ (większy gradient)\n",
    "6. __exploding gradient__\n",
    "  * w algorytmie backpropagation delta mnożona jest przez macierz wag (przypomnieć sobie wzór)\n",
    "  * jeśli wagi są duże, to delta może zostać zwiększona\n",
    "  * znowu, im głębiej, tym bardziej te efekty mogą się nawarstwiać\n",
    "  * w najgłębszej warstwie gradient może być ogromny\n",
    "    * całkowity brak stabilności uczenia\n",
    "    * mała zmiana inputu zupełnie zmieni output sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Różne możliwe rozwiązania\n",
    "<img style=\"float: right;\" src=\"ml_figures/sgd-training-rate.jpg\" width=400>\n",
    "\n",
    "### Znormalizowany input\n",
    "\n",
    "1. transformacja zbioru treningowego\n",
    "  * trzeba też transformować zbiór testowy!\n",
    "2. cel - każda współrzędna inputu ma mieć średnią zero i wariancję jeden\n",
    "  * estymujemy średnią i wariancję dla każdej kolumny zbioru treningowego\n",
    "  * odejmujemy średnią, dzielimy przez pierwiastek z wariancji\n",
    "  * zapamiętujemy wektor średnich i wektor wariancji, aby móc transformować przyszłe dane\n",
    "  * __nie powinno się estymować tych parametrów przy użyciu zbioru testowego__\n",
    "    * zbiór testowy ma symulować dane z przyszłości\n",
    "    * nie możemy używać go podczas uczenia\n",
    "    * normalizacja jest częścią procesu uczenia sieci\n",
    "3. jeszcze lepiej działałaby __dekorelacja cech__\n",
    "  * jeśli dwie kolumny cech są skorelowane, to niosą mniej informacji\n",
    "  * w skrajnym przypadku dwie kolumny cech są identyczne\n",
    "    * największa korelacja\n",
    "  * dekorelacja wymaga obliczenia i zapamiętania macierzy kowariancji\n",
    "    * kowariancja każdej pary cech\n",
    "    * złożoność kwadratowa względem $D$ (obliczenie wariancji ma złożoność liniową)\n",
    "    * zazwyczaj się tego nie robi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicjalizacja wag sieci\n",
    "\n",
    "1. inicjalizacja zerami utrudnia uczenie\n",
    "2. trzeba inicjalizować losowymi wartościami\n",
    "  * trzeba mądrze losować\n",
    "3. [[Xavier Glorot, Yoshua Bengio, \"Understanding the difficulty of training deep feedforward neural networks\"]](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "  * kod initializer'a w notebooku na ćwiczeniach\n",
    "4. pomysł polega na tym, aby (przynajmniej początkowo) wszystkie warstwy dostawały znormalizowany input\n",
    "  * zaawansowane rozszerzenie - _batch normalization_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uczenie batchowe\n",
    "\n",
    "1. liczymy gradient przy pomocy mini-batch'a (np. 64 przykłady treningowe)\n",
    "2. niedokładne przybliżenie gradientu dla całego zbioru treningowego\n",
    "  * szybsze\n",
    "  * zmniejsza overfitting\n",
    "    * przez dodatkową losowość model ma problem z zapamiętaniem zbioru treningowego\n",
    "3. badania nad związkiem rozmiaru mini-batch'a z learning rate\n",
    "  * zmniejszanie learning rate ma podobny efekt jak zwiększanie batch size\n",
    "  * i na odwrót"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularyzacja\n",
    "\n",
    "1. dodajemy do kosztu dodatkowy składnik\n",
    "2. regularyzacja L2\n",
    "  * suma kwadratów wszystkich parametrów ze wszystkich warstw sieci\n",
    "  * przemnożona przez hiperparametr $\\lambda$\n",
    "3. regularyzacja ma za zadanie zniechęcić sieć do nauczenia się skrajnie dużych wartości dla niektórych parametrów"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
